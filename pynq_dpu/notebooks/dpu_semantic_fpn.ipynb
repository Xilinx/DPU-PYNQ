{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fca39180-b0bc-45d4-be84-9e36e6153e51",
   "metadata": {},
   "source": [
    "# DPU example: Semantic-FPN\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3016107-ad69-4752-82de-39f69287abae",
   "metadata": {},
   "source": [
    "## Aim/s\n",
    "* This notebooks shows an example of DPU applications. The application, as well as the DPU IP, is pulled from the official \n",
    "[Vitis AI Github Repository](https://github.com/Xilinx/Vitis-AI).\n",
    "* Description: Semantic-FPN for segmentation on Cityscapes.\n",
    "* Input size: 1024x512\n",
    "* Task: Segmentation\n",
    "\n",
    "## References\n",
    "* [Vitis AI Github Repository](https://www.xilinx.com/products/design-tools/vitis/vitis-ai.html).\n",
    "\n",
    "## Last revised\n",
    "* December 14, 2023\n",
    "    * Initial revision. MM, Alpha Data Parallel Systems Ltd.\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468bddfe-d28e-47c9-b9dc-ccfb136b1728",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import glob\n",
    "import time\n",
    "import numpy as np\n",
    "import IPython\n",
    "from PIL import Image\n",
    "\n",
    "from pynq_dpu import DpuOverlay"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc447f4c-5abb-4fb9-8127-b528f5111ca4",
   "metadata": {},
   "source": [
    "## 1. Prepare the overlay\n",
    "We will download the overlay onto the board and prepare the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9803d26-f3d4-4e44-8bf7-de75f85dc9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "ol = DpuOverlay('dpu.bit')\n",
    "ol.load_model('pt_SemanticFPN-mobilenetv2.xmodel')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0368ac3f-877e-4b02-8c2a-bc7cf97c70ef",
   "metadata": {},
   "source": [
    "## 2. Utility functions\n",
    "\n",
    "In this section, we will prepare a few functions and the input data for later use.\n",
    "\n",
    "First we need to define helper functions to prepare the input images for the model. At this\n",
    "stage we will also define a post-processing function to decode the output of the model and\n",
    "apply a color palette to annotated the semantic information of the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd0c52a-3178-4adf-b2e5-ba56f8b20ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "MEAN = (123.675, 116.28, 103.53)\n",
    "STD = (58.395, 57.12, 57.375)\n",
    "\n",
    "def preprocess(image, size=(1024, 512)):\n",
    "    '''\n",
    "    Function to prepare the input for the Semantic-FPN model.\n",
    "    The input images are first converted to the RGB color space,\n",
    "    then resized and finally shifted and scaled to standardise the\n",
    "    color channel values.\n",
    "    \n",
    "    Args:\n",
    "        image: 3 dimensional array corresponding to the input BGR image.\n",
    "        size (optional): (new_width, new_height) tuple corresponding to \n",
    "                         the size of the input the Semantic-FPN model is\n",
    "                         expecting. Set to 1024x512 by default.\n",
    "        \n",
    "    Returns:\n",
    "        3 dimensional array corresponding to the input for Semantic-FPN.\n",
    "        The array has a shape of (new_height, new_width, 3).\n",
    "    '''\n",
    "    rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    resized_image = cv2.resize(rgb_image, size, interpolation=cv2.INTER_LINEAR)\n",
    "    processed_image = (resized_image - MEAN) / STD\n",
    "    return processed_image.astype(np.float32)\n",
    "\n",
    "def get_palette():\n",
    "    '''\n",
    "    Function to generate the color palette to annotate the semantic information.\n",
    "    \n",
    "    Returns:\n",
    "        2 dimensional array of the shape (256, 3) corresponding to the color\n",
    "        values for each channel of the colorised image. While only 19 classes\n",
    "        are predicted by the model, the cv2.LUT function requires a palette for\n",
    "        all 256 possible values of the 8-bit unsigned integers in the mask.\n",
    "    '''\n",
    "    palette = [128, 64, 128,\n",
    "               244, 35, 232,\n",
    "               70, 70, 70,\n",
    "               102, 102, 156,\n",
    "               190, 153, 153,\n",
    "               153, 153, 153,\n",
    "               250, 170, 30,\n",
    "               220, 220, 0,\n",
    "               107, 142, 35,\n",
    "               152, 251, 152,\n",
    "               70, 130, 180,\n",
    "               220, 20, 60,\n",
    "               255, 0, 0,\n",
    "               0, 0, 142,\n",
    "               0, 0, 70,\n",
    "               0, 60, 100,\n",
    "               0, 80, 100,\n",
    "               0, 0, 230,\n",
    "               119, 11, 32]\n",
    "\n",
    "    palette = palette + [0] * (256 * 3 - len(palette))\n",
    "    \n",
    "    return np.array(palette, dtype=np.uint8).reshape((-1, 3))\n",
    "\n",
    "PALETTE = get_palette()\n",
    "\n",
    "def colorise_mask(mask):\n",
    "    '''\n",
    "    Function to colorise the output mask of the model.\n",
    "    \n",
    "    Input:\n",
    "        mask: 2 dimensional array corresponding to the color index of each\n",
    "              pixel classified during segmentation.\n",
    "    \n",
    "    Returns:\n",
    "        3 dimensional array corresponding to the image colorised according to\n",
    "        the provided mask.\n",
    "    '''\n",
    "    uint_mask = mask.astype(np.uint8)\n",
    "    channels = [cv2.LUT(uint_mask, PALETTE[:, i]) for i in range(3)]\n",
    "    return np.dstack(channels)\n",
    "\n",
    "def postprocess(output_tensors):\n",
    "    '''\n",
    "    Function to decode the output of the model and colorise the segmenation\n",
    "    results.\n",
    "    \n",
    "    Input:\n",
    "        output_tensors: buffers allocated for the DPU to store the output of\n",
    "                        the model.\n",
    "    \n",
    "    Returns:\n",
    "        3 dimensional array corresponding to the image colorised according to\n",
    "        the output of the model.\n",
    "    '''\n",
    "    output = output_tensors[0][0]\n",
    "    seg_pred = np.argmax(output, axis=2)\n",
    "    return colorise_mask(seg_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a34e5f6-b364-4c1f-a862-df02e16d7c81",
   "metadata": {},
   "source": [
    "Finally, use a search pattern to collect all the `png` files for segmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1082ec32-ef15-465b-928a-8c661800987c",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_pattern = r'img/segm/*.png'\n",
    "original_images = glob.glob(search_pattern)\n",
    "total_images = len(original_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60f3a57-59f1-4bbf-a31b-ff6e50cc26ed",
   "metadata": {},
   "source": [
    "## 3. Use VART\n",
    "\n",
    "Now we should be able to use VART to do semantic segmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ffc712-f94f-46d4-99f6-b56715b64bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dpu = ol.runner\n",
    "\n",
    "input_tensors = dpu.get_input_tensors()\n",
    "output_tensors = dpu.get_output_tensors()\n",
    "\n",
    "shape_in = input_tensors[0].dims\n",
    "shape_out = output_tensors[0].dims"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f429d560-2109-4ebb-9a84-de116fa1062d",
   "metadata": {},
   "source": [
    "We can define a few buffers to store input and output data. They will be reused during \n",
    "multiple runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0ee885-7136-4aee-97a1-7399735ace4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = [np.empty(shape_in, dtype=np.float32, order=\"C\")]\n",
    "output_data = [np.empty(shape_out, dtype=np.float32, order=\"C\")]\n",
    "\n",
    "image = input_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ce8faf-829f-4ece-9374-f1cd69e38d3d",
   "metadata": {},
   "source": [
    "Remember that we have a list of `original_images`. We can no define a new function `run()` which takes the image index as the input, prepares the image for the model, executes the DPU for the given input and finally post-processes the output of the DPU. With the argument \n",
    "`display` set to `True`, the colorised output of the model is displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0392891-d799-449c-ab58-aae4fb5b6b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(image_index, display=False):\n",
    "    '''\n",
    "    Function to handle a single input for the DPU and process the output of the run.\n",
    "    \n",
    "    Args:\n",
    "        image_index: int corresonding to the index of the collected images to be processed.\n",
    "        display (optional): boolean flag set to annotate and display the input image.\n",
    "        \n",
    "    Returns:\n",
    "        None, the annotated image is displayed when the display flag is set.\n",
    "        \n",
    "    Raises:\n",
    "        AssertionError: when the provided image_index is larger than the number of potential\n",
    "                        input images found.\n",
    "    \n",
    "    '''\n",
    "    assert image_index < total_images, \\\n",
    "    f'Please specify an image index less than {total_images}. Index provided: {image_index}.'\n",
    "    \n",
    "    # Read input image\n",
    "    input_image = cv2.imread(original_images[image_index])\n",
    "    \n",
    "    # Pre-processing\n",
    "    image_data = preprocess(input_image)\n",
    "    \n",
    "    # Fetch data to DPU and trigger it\n",
    "    image[...] = image_data\n",
    "    job_id = dpu.execute_async(input_data, output_data)\n",
    "    dpu.wait(job_id)\n",
    "    \n",
    "    # Post-processing\n",
    "    output_image = postprocess(output_data)\n",
    "    \n",
    "    if display:\n",
    "        IPython.display.display(Image.fromarray(output_image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa70895e-55b4-4cf2-ba17-4380cd23dc48",
   "metadata": {},
   "outputs": [],
   "source": [
    "run(0, display=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac328e7d-a7d3-4a99-b761-d1ebf478e5ed",
   "metadata": {},
   "source": [
    "We can also run it for multiple images as shown below. In this example we have only used 1 thread; in principle, users should be able to boost the performance by employing more threads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ca2fa1-46f5-47fe-9e0d-337c6da9537b",
   "metadata": {},
   "outputs": [],
   "source": [
    "time1 = time.time()\n",
    "[run(i) for i in range(total_images)]\n",
    "time2 = time.time()\n",
    "fps = total_images/(time2-time1)\n",
    "print(\"Performance: {} FPS\".format(fps))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731fc7a8-4e38-4071-8180-da9ada0fbb89",
   "metadata": {},
   "source": [
    "We will need to remove references to `vart.Runner` and let Python garbage-collect the unused graph objects. This will make sure we can run other notebooks without any issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2dfe33d-0937-4771-b69f-b2f454c9e326",
   "metadata": {},
   "outputs": [],
   "source": [
    "del dpu\n",
    "del ol"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ff51c3-1664-4bca-b4c2-83297b7f2b22",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "Copyright (C) 2021 Xilinx, Inc\n",
    "\n",
    "SPDX-License-Identifier: Apache-2.0 License\n",
    "\n",
    "----\n",
    "\n",
    "----"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
