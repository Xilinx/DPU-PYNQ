{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c1bdc8a-8956-44aa-90c8-d0c8b94db9d8",
   "metadata": {},
   "source": [
    "# DPU example: RefineDet\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268569cc-1804-400b-8def-a0247fd2a48b",
   "metadata": {},
   "source": [
    "## Aim/s\n",
    "* This notebooks shows an example of DPU applications. The application, as well as the DPU IP, is pulled from the official \n",
    "[Vitis AI Github Repository](https://github.com/Xilinx/Vitis-AI).\n",
    "* Description: RefineDet for object detection on VOC.\n",
    "* Input size: 320x320\n",
    "* Task: Object detection\n",
    "\n",
    "## References\n",
    "* [Vitis AI Github Repository](https://www.xilinx.com/products/design-tools/vitis/vitis-ai.html).\n",
    "\n",
    "## Last revised\n",
    "* Dec 14, 2023\n",
    "    * Initial revision. MM, Alpha Data Parallel Systems Ltd.\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14eb8aed-7b41-413f-b2d5-5dd2d492815d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import time\n",
    "import glob\n",
    "import random\n",
    "import colorsys\n",
    "import numpy as np\n",
    "import IPython\n",
    "from PIL import Image\n",
    "\n",
    "from pynq_dpu import DpuOverlay\n",
    "\n",
    "# Import helper functions form refinedet_utils.py\n",
    "# Original file available as utils_tf.py in the Vitis AI Github repository.\n",
    "from refinedet_utils import pboxes_vgg_voc, Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bdee6de-47a5-40b1-b8fe-ff1282470dfc",
   "metadata": {},
   "source": [
    "## 1. Prepare the overlay\n",
    "We will download the overlay onto the board and prepare the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664a261b-986d-4251-b158-0c80462fccb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ol = DpuOverlay('dpu.bit')\n",
    "ol.load_model('refinedet_voc_tf.xmodel')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d36542b-7822-4c6c-86bc-5bbd8eddfd08",
   "metadata": {},
   "source": [
    "## 2. Utility functions\n",
    "\n",
    "In this section, we will prepare a few functions and the input data for later use.\n",
    "\n",
    "First we need to load the class labels for the VOC dataset and determine the colors for annotation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a619a9d-d8bb-41d1-b955-934e3b47ee04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class(classes_path):\n",
    "    '''\n",
    "    Function to parse the VOC classes file to get the classification information\n",
    "    for the model.\n",
    "    \n",
    "    Input:\n",
    "        classes_path: string corresponding to the path to the class labels file.\n",
    "        \n",
    "    Returns:\n",
    "        List of strings corresponding to the individual class labels.\n",
    "    \n",
    "    '''\n",
    "    with open(classes_path) as f:\n",
    "        class_names = f.readlines()\n",
    "    class_names = [c.strip() for c in class_names]\n",
    "    return class_names\n",
    "\n",
    "# Parse the classes file\n",
    "classes_path = \"img/voc_classes.txt\"\n",
    "class_names = get_class(classes_path)\n",
    "\n",
    "# Generate colors for each class\n",
    "num_classes = len(class_names)\n",
    "hsv_tuples = [(1.0 * x / num_classes, 1., 1.) for x in range(num_classes)]\n",
    "colors = list(map(lambda x: colorsys.hsv_to_rgb(*x), hsv_tuples))\n",
    "colors = list(map(lambda x: \n",
    "                  (int(x[0] * 255), int(x[1] * 255), int(x[2] * 255)), \n",
    "                  colors))\n",
    "random.seed(0)\n",
    "random.shuffle(colors)\n",
    "random.seed(None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a73edfb-85ef-4b30-8e23-c173d6c16050",
   "metadata": {},
   "source": [
    "Next we need to define helper functions to prepare the input images for the model.\n",
    "At this stage we will also define a post-processing function to decode the output\n",
    "of the model and draw the bounding boxes.\n",
    "\n",
    "The post-processing step for the RefineDet model is quite complex. To keep this\n",
    "example notebook clear, the helper functions for post-processing are defined in\n",
    "the [refinedet_utils.py](refinedet_utils.py) file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d484de-092f-4b5c-a770-e92244dac606",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(image, size=(320, 320)):\n",
    "    '''\n",
    "    Function to prepare the input for the RefineDet model.\n",
    "    The input images are first resized then mean subtraction is used to\n",
    "    shift the colour channel values.\n",
    "    \n",
    "    Args:\n",
    "        image: 3 dimensional array corresponding to the input BGR image.\n",
    "        size (optional): (new_width, new_height) tuple corresponding to \n",
    "                         the size of the input the RefineDet model is\n",
    "                         expecting. Set to 320x320 by default.\n",
    "                         \n",
    "    Returns:\n",
    "        3 dimensional array corresponding to the input for RefineDet. The\n",
    "        array has a shape of (new_height, new_width, 3).\n",
    "    '''\n",
    "    \n",
    "    image = cv2.resize(image, size)\n",
    "    image = image.astype('float32')\n",
    "    R_MEAN = 123.68\n",
    "    G_MEAN = 116.78 \n",
    "    B_MEAN = 103.94\n",
    "    mean = np.array([B_MEAN, G_MEAN, R_MEAN], dtype=np.float32)\n",
    "    image = image - mean\n",
    "    \n",
    "    return image\n",
    "\n",
    "def postprocess(image, output_tensors, display, score_threshold=0.5):\n",
    "    '''\n",
    "    Function to decode the output of the model and annotate the original\n",
    "    image with the bounding boxes, predicted labels and confidence score.\n",
    "    \n",
    "    Args:\n",
    "        image: 3 dimensional array corresponding to the original input image.\n",
    "        output_tensors: buffers allocated for the DPU to store the output of\n",
    "                        the model.\n",
    "        display: boolean flag set to annotate and display the input image.\n",
    "        score_threshold (optional): float corresponding to the minimum \n",
    "                        confidence in the prediction for annotation.\n",
    "                        \n",
    "    Returns:\n",
    "        None, the annotated image is displayed when the display flag is set.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    original_height, original_width, _ = image.shape\n",
    "    bbox_thick = int(0.6 * (original_height + original_width) / 600)\n",
    "    \n",
    "    arm_loc = output_tensors[0] # (1, 8500, 4)\n",
    "    arm_cls = output_tensors[1] # (1, 8500, 2)\n",
    "    odm_loc = output_tensors[2] # (1, 8500, 4)\n",
    "    odm_cls = output_tensors[3] # (1, 8500, 21)\n",
    "    \n",
    "    pboxes = pboxes_vgg_voc()\n",
    "    encoder = Encoder(pboxes)\n",
    "    \n",
    "    loc, label, prob = encoder.decode_batch(arm_cls, arm_loc, odm_cls, odm_loc, 0.45, 200, device=0)[0]\n",
    "    valid_mask = np.logical_and((loc[:, 2] - loc[:, 0] > 0), (loc[:, 3] - loc[:, 1] > 0))\n",
    "    \n",
    "    for i in range(prob.shape[0]-1, -1,-1):\n",
    "        if not valid_mask[i]: continue\n",
    "            \n",
    "        score = prob[i]\n",
    "        if score < score_threshold: break\n",
    "            \n",
    "        xmin = loc[i][0] * original_width\n",
    "        ymin = loc[i][1] * original_height\n",
    "        xmax = loc[i][2] * original_width\n",
    "        ymax = loc[i][3] * original_height\n",
    "        class_id = int(label[i]) - 1\n",
    "        \n",
    "        if not display: continue\n",
    "        \n",
    "        color = colors[class_id]\n",
    "        text = f'{class_names[label[i] - 1]}: {prob[i]:.4f}'\n",
    "        \n",
    "        cv2.rectangle(image, (int(xmin), int(ymin)), (int(xmax), int(ymax)), color, bbox_thick)\n",
    "        \n",
    "        cv2.putText(image,\n",
    "                    text,\n",
    "                    (int((xmax - xmin)/2), int((ymax-ymin)/2)),\n",
    "                    fontFace=cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                    fontScale=0.6,\n",
    "                    color=color,\n",
    "                    thickness=bbox_thick)\n",
    "        \n",
    "    if display:\n",
    "        IPython.display.display(Image.fromarray(image))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f0651f-e2de-4e2e-9d37-c01a86ba2722",
   "metadata": {},
   "source": [
    "Finally, use a search pattern to collect all the `JPEG` files for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7eccfd-ff69-4ea8-8daf-fad3a31bf3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_pattern = f'img/*.JPEG'\n",
    "original_images = glob.glob(search_pattern)\n",
    "total_images = len(original_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12ce21a-e59d-4bc9-b1b2-9a980580d411",
   "metadata": {},
   "source": [
    "## 3. Use VART\n",
    "\n",
    "Now we should be able to use VART to do object detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20dfbad-888e-440d-a8fb-908f44d176d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dpu = ol.runner\n",
    "\n",
    "input_tensors = dpu.get_input_tensors()\n",
    "output_tensors = dpu.get_output_tensors()\n",
    "\n",
    "shape_in = input_tensors[0].dims\n",
    "shape_out0 = output_tensors[0].dims\n",
    "shape_out1 = output_tensors[1].dims\n",
    "shape_out2 = output_tensors[2].dims\n",
    "shape_out3 = output_tensors[3].dims"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef7438f-f18a-4200-a415-0fc61a961667",
   "metadata": {},
   "source": [
    "We can define a few buffers to store input and output data. They will be reused during multiple runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd15e3ee-2f85-4896-9f0c-2caa24f85e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = [np.empty(shape_in, dtype=np.float32, order=\"C\")]\n",
    "output_data = [np.empty(shape_out0, dtype=np.float32, order=\"C\"), \n",
    "               np.empty(shape_out1, dtype=np.float32, order=\"C\"),\n",
    "               np.empty(shape_out2, dtype=np.float32, order=\"C\"),\n",
    "               np.empty(shape_out3, dtype=np.float32, order=\"C\")]\n",
    "\n",
    "image = input_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc31fe7-0a4f-4bbd-b2e8-af2f814d3163",
   "metadata": {},
   "source": [
    "Remember that we have a list of `original_images`. We can no define a new function `run()` which takes the image index as the input, prepares the image for the model, executes the DPU for the given input and finally post-processes the output of the DPU. With the argument \n",
    "`display` set to `True`, the original image with the bounding boxes and class labels and \n",
    "confidence scores can be rendered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65d0061-8540-46c1-a544-3d00fa7dd13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(image_index, display=False):\n",
    "    '''\n",
    "    Function to handle a single input for the DPU and process the output of the run.\n",
    "    \n",
    "    Args:\n",
    "        image_index: int corresonding to the index of the collected images to be processed.\n",
    "        display (optional): boolean flag set to annotate and display the input image.\n",
    "        \n",
    "    Returns:\n",
    "        None, the annotated image is displayed when the display flag is set.\n",
    "        \n",
    "    Raises:\n",
    "        AssertionError: when the provided image_index is larger than the number of potential\n",
    "                        input images found.\n",
    "    \n",
    "    '''\n",
    "    assert image_index < total_images, \\\n",
    "    f'Please specify an image index less than {total_images}. Index provided: {image_index}.'\n",
    "    \n",
    "    # Read input image\n",
    "    input_image = cv2.imread(original_images[image_index])\n",
    "    \n",
    "    # Pre-process image\n",
    "    image_data = preprocess(input_image)\n",
    "    \n",
    "    # Fetch data to DPU and trigger it\n",
    "    image[0, ...] = image_data\n",
    "    job_id = dpu.execute_async(input_data, output_data)\n",
    "    dpu.wait(job_id)\n",
    "    \n",
    "    # Post-process result\n",
    "    postprocess(input_image, output_data, display, score_threshold=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5054ee-a60c-4be6-a218-fa94592f543c",
   "metadata": {},
   "source": [
    "Let's run it for 1 image and display the annotated image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb45284-6fb0-475c-98af-d8e41b44c1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "run(0, display=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ebd8d8-f020-4444-bda6-00790a34ca4a",
   "metadata": {},
   "source": [
    "We can also run it for multiple images as shown below. In this example we have only used 1 thread; in principle, users should be able to boost the performance by employing more threads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6ba486-ab82-4730-b30a-7c053fffa154",
   "metadata": {},
   "outputs": [],
   "source": [
    "time1 = time.time()\n",
    "[run(i) for i in range(total_images)]\n",
    "time2 = time.time()\n",
    "fps = total_images/(time2-time1)\n",
    "print(\"Performance: {} FPS\".format(fps))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d1d65c-a33d-4c3e-b550-a458683062ad",
   "metadata": {},
   "source": [
    "We will need to remove references to `vart.Runner` and let Python garbage-collect the unused graph objects. This will make sure we can run other notebooks without any issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf241ac-0f94-4055-be79-1d4fa1c4b26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "del dpu\n",
    "del ol"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2d276f-574f-4b68-8794-6d93f4c133b8",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "Copyright (C) 2021 Xilinx, Inc\n",
    "\n",
    "SPDX-License-Identifier: Apache-2.0 License\n",
    "\n",
    "----\n",
    "\n",
    "----"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
